[sms]# echo ${sms_ip} ${sms_name} >> /etc/hosts
[sms]# systemctl disable firewalld
[sms]# systemctl stop firewalld
yum install http://build.openhpc.community/OpenHPC:/1.3/CentOS_7/x86_64/ohpc-release-1.3-1.el7.x86_64.rpm
# Install base meta-packages
[sms]# yum -y install ohpc-base
[sms]# yum -y install ohpc-warewulf
# [sms]# ipmitool -E -I lanplus -H ${bmc_ipaddr} -U root chassis bootdev pxe options=persistent
[sms]# systemctl enable ntpd.service
[sms]# echo "server ${ntp_server}" >> /etc/ntp.conf
[sms]# systemctl restart ntpd
# Install slurm server meta-package
[sms]# yum -y install ohpc-slurm-server

# Identify resource manager hostname on master host
# [sms]# perl -pi -e "s/ControlMachine=\S+/ControlMachine=${sms_name}/" /etc/slurm/slurm.conf

# [sms]# yum -y groupinstall "InfiniBand Support"
# [sms]# yum -y install infinipath-psm
# Load IB drivers
# [sms]# systemctl start rdma

# [sms]# cp /opt/ohpc/pub/examples/network/centos/ifcfg-ib0 /etc/sysconfig/network-scripts
# Define local IPoIB address and netmask
# [sms]# perl -pi -e "s/master_ipoib/${sms_ipoib}/" /etc/sysconfig/network-scripts/ifcfg-ib0
# [sms]# perl -pi -e "s/ipoib_netmask/${ipoib_netmask}/" /etc/sysconfig/network-scripts/ifcfg-ib0
# Initiate ib0
# [sms]# ifup ib0

# Configure Warewulf provisioning to use desired internal interface
[sms]# perl -pi -e "s/device = eth1/device = ${sms_eth_internal}/" /etc/warewulf/provision.conf
# Enable tftp service for compute node image distribution
[sms]# perl -pi -e "s/^\s+disable\s+= yes/ disable = no/" /etc/xinetd.d/tftp
# Enable internal interface for provisioning
[sms]# ifconfig ${sms_eth_internal} ${sms_ip} netmask ${internal_netmask} up
# Restart/enable relevant services to support provisioning
[sms]# systemctl restart xinetd
[sms]# systemctl enable mariadb.service
[sms]# systemctl restart mariadb
[sms]# systemctl enable httpd.service
[sms]# systemctl restart httpd
[sms]# systemctl enable dhcpd.service




initial BOS image
The OpenHPC build of Warewulf includes specic enhancements enabling support for CentOS7.4. The
following steps illustrate the process to build a minimal, default image for use with Warewulf. We begin
by dening a directory structure on the master host that will represent the root lesystem of the compute
node. The default location for this example is in /opt/ohpc/admin/images/centos7.4.


Warewulf is congured by default to access an external repository (mirror.centos.org) during the wwmkchroot
process. If the master host cannot reach the public CentOS mirrors, or if you prefer to access a locally
cached mirror, set the $fYUM MIRRORg environment variable to your desired repo location prior to running the
wwmkchroot command below. For example:
# Override default OS repository (optional) - set YUM_MIRROR variable to desired repo location
[sms]# export YUM_MIRROR=${BOS_MIRROR}


# Define chroot location
[sms]# export CHROOT=/opt/ohpc/admin/images/centos7.4
# Build initial chroot image
[sms]# wwmkchroot centos-7 $CHROOT

# Install compute node base meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-base-compute
To access the remote repositories by hostname (and not IP addresses), the chroot environment needs to
be updated to enable DNS resolution. Assuming that the master host has a working DNS conguration in
place, the chroot environment can be updated with a copy of the conguration as follows:
[sms]# cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
Now, we can include additional components to the compute instance using yum to install into the chroot
location dened previously:
# Add Slurm client support meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-slurm-client
# Add Network Time Protocol (NTP) support
[sms]# yum -y --installroot=$CHROOT install ntp
# Add kernel drivers
[sms]# yum -y --installroot=$CHROOT install kernel
# Include modules user environment
[sms]# yum -y --installroot=$CHROOT install lmod-ohpc





Warewulf is congured by default to access an external repository (mirror.centos.org) during the wwmkchroot
process. If the master host cannot reach the public CentOS mirrors, or if you prefer to access a locally
cached mirror, set the $fYUM MIRRORg environment variable to your desired repo location prior to running the
wwmkchroot command below. For example:
# Override default OS repository (optional) - set YUM_MIRROR variable to desired repo location
[sms]# export YUM_MIRROR=${BOS_MIRROR}


# Define chroot location
[sms]# export CHROOT=/opt/ohpc/admin/images/centos7.4
# Build initial chroot image
[sms]# wwmkchroot centos-7 $CHROOT

# Install compute node base meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-base-compute
To access the remote repositories by hostname (and not IP addresses), the chroot environment needs to
be updated to enable DNS resolution. Assuming that the master host has a working DNS conguration in
place, the chroot environment can be updated with a copy of the conguration as follows:
[sms]# cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
Now, we can include additional components to the compute instance using yum to install into the chroot
location dened previously:
# Add Slurm client support meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-slurm-client
# Add Network Time Protocol (NTP) support
[sms]# yum -y --installroot=$CHROOT install ntp
# Add kernel drivers
[sms]# yum -y --installroot=$CHROOT install kernel
# Include modules user environment
[sms]# yum -y --installroot=$CHROOT install lmod-ohpc





Warewulf is congured by default to access an external repository (mirror.centos.org) during the wwmkchroot
process. If the master host cannot reach the public CentOS mirrors, or if you prefer to access a locally
cached mirror, set the $fYUM MIRRORg environment variable to your desired repo location prior to running the
wwmkchroot command below. For example:
# Override default OS repository (optional) - set YUM_MIRROR variable to desired repo location
[sms]# export YUM_MIRROR=${BOS_MIRROR}


# Define chroot location
[sms]# export CHROOT=/opt/ohpc/admin/images/centos7.4
# Build initial chroot image
[sms]# wwmkchroot centos-7 $CHROOT

# Install compute node base meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-base-compute
To access the remote repositories by hostname (and not IP addresses), the chroot environment needs to
be updated to enable DNS resolution. Assuming that the master host has a working DNS conguration in
place, the chroot environment can be updated with a copy of the conguration as follows:
[sms]# cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
Now, we can include additional components to the compute instance using yum to install into the chroot
location dened previously:
# Add Slurm client support meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-slurm-client
# Add Network Time Protocol (NTP) support
[sms]# yum -y --installroot=$CHROOT install ntp
# Add kernel drivers
[sms]# yum -y --installroot=$CHROOT install kernel
# Include modules user environment
[sms]# yum -y --installroot=$CHROOT install lmod-ohpc





Warewulf is congured by default to access an external repository (mirror.centos.org) during the wwmkchroot
process. If the master host cannot reach the public CentOS mirrors, or if you prefer to access a locally
cached mirror, set the $fYUM MIRRORg environment variable to your desired repo location prior to running the
wwmkchroot command below. For example:
# Override default OS repository (optional) - set YUM_MIRROR variable to desired repo location
[sms]# export YUM_MIRROR=${BOS_MIRROR}


# Define chroot location
[sms]# export CHROOT=/opt/ohpc/admin/images/centos7.4
# Build initial chroot image
[sms]# wwmkchroot centos-7 $CHROOT

# Install compute node base meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-base-compute
To access the remote repositories by hostname (and not IP addresses), the chroot environment needs to
be updated to enable DNS resolution. Assuming that the master host has a working DNS conguration in
place, the chroot environment can be updated with a copy of the conguration as follows:
[sms]# cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
Now, we can include additional components to the compute instance using yum to install into the chroot
location dened previously:
# Add Slurm client support meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-slurm-client
# Add Network Time Protocol (NTP) support
[sms]# yum -y --installroot=$CHROOT install ntp
# Add kernel drivers
[sms]# yum -y --installroot=$CHROOT install kernel
# Include modules user environment
[sms]# yum -y --installroot=$CHROOT install lmod-ohpc





Warewulf is congured by default to access an external repository (mirror.centos.org) during the wwmkchroot
process. If the master host cannot reach the public CentOS mirrors, or if you prefer to access a locally
cached mirror, set the $fYUM MIRRORg environment variable to your desired repo location prior to running the
wwmkchroot command below. For example:
# Override default OS repository (optional) - set YUM_MIRROR variable to desired repo location
[sms]# export YUM_MIRROR=${BOS_MIRROR}


# Define chroot location
[sms]# export CHROOT=/opt/ohpc/admin/images/centos7.4
# Build initial chroot image
[sms]# wwmkchroot centos-7 $CHROOT

# Install compute node base meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-base-compute
To access the remote repositories by hostname (and not IP addresses), the chroot environment needs to
be updated to enable DNS resolution. Assuming that the master host has a working DNS conguration in
place, the chroot environment can be updated with a copy of the conguration as follows:
[sms]# cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
Now, we can include additional components to the compute instance using yum to install into the chroot
location dened previously:
# Add Slurm client support meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-slurm-client
# Add Network Time Protocol (NTP) support
[sms]# yum -y --installroot=$CHROOT install ntp
# Add kernel drivers
[sms]# yum -y --installroot=$CHROOT install kernel
# Include modules user environment
[sms]# yum -y --installroot=$CHROOT install lmod-ohpc





Warewulf is congured by default to access an external repository (mirror.centos.org) during the wwmkchroot
process. If the master host cannot reach the public CentOS mirrors, or if you prefer to access a locally
cached mirror, set the $fYUM MIRRORg environment variable to your desired repo location prior to running the
wwmkchroot command below. For example:
# Override default OS repository (optional) - set YUM_MIRROR variable to desired repo location
[sms]# export YUM_MIRROR=${BOS_MIRROR}


# Define chroot location
[sms]# export CHROOT=/opt/ohpc/admin/images/centos7.4
# Build initial chroot image
[sms]# wwmkchroot centos-7 $CHROOT

# Install compute node base meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-base-compute
To access the remote repositories by hostname (and not IP addresses), the chroot environment needs to
be updated to enable DNS resolution. Assuming that the master host has a working DNS conguration in
place, the chroot environment can be updated with a copy of the conguration as follows:
[sms]# cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
Now, we can include additional components to the compute instance using yum to install into the chroot
location dened previously:
# Add Slurm client support meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-slurm-client
# Add Network Time Protocol (NTP) support
[sms]# yum -y --installroot=$CHROOT install ntp
# Add kernel drivers
[sms]# yum -y --installroot=$CHROOT install kernel
# Include modules user environment
[sms]# yum -y --installroot=$CHROOT install lmod-ohpc





Warewulf is congured by default to access an external repository (mirror.centos.org) during the wwmkchroot
process. If the master host cannot reach the public CentOS mirrors, or if you prefer to access a locally
cached mirror, set the $fYUM MIRRORg environment variable to your desired repo location prior to running the
wwmkchroot command below. For example:
# Override default OS repository (optional) - set YUM_MIRROR variable to desired repo location
[sms]# export YUM_MIRROR=${BOS_MIRROR}


# Define chroot location
[sms]# export CHROOT=/opt/ohpc/admin/images/centos7.4
# Build initial chroot image
[sms]# wwmkchroot centos-7 $CHROOT

# Install compute node base meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-base-compute
To access the remote repositories by hostname (and not IP addresses), the chroot environment needs to
be updated to enable DNS resolution. Assuming that the master host has a working DNS conguration in
place, the chroot environment can be updated with a copy of the conguration as follows:
[sms]# cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
Now, we can include additional components to the compute instance using yum to install into the chroot
location dened previously:
# Add Slurm client support meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-slurm-client
# Add Network Time Protocol (NTP) support
[sms]# yum -y --installroot=$CHROOT install ntp
# Add kernel drivers
[sms]# yum -y --installroot=$CHROOT install kernel
# Include modules user environment
[sms]# yum -y --installroot=$CHROOT install lmod-ohpc





Warewulf is congured by default to access an external repository (mirror.centos.org) during the wwmkchroot
process. If the master host cannot reach the public CentOS mirrors, or if you prefer to access a locally
cached mirror, set the $fYUM MIRRORg environment variable to your desired repo location prior to running the
wwmkchroot command below. For example:
# Override default OS repository (optional) - set YUM_MIRROR variable to desired repo location
[sms]# export YUM_MIRROR=${BOS_MIRROR}


# Define chroot location
[sms]# export CHROOT=/opt/ohpc/admin/images/centos7.4
# Build initial chroot image
[sms]# wwmkchroot centos-7 $CHROOT

# Install compute node base meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-base-compute
To access the remote repositories by hostname (and not IP addresses), the chroot environment needs to
be updated to enable DNS resolution. Assuming that the master host has a working DNS conguration in
place, the chroot environment can be updated with a copy of the conguration as follows:
[sms]# cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
Now, we can include additional components to the compute instance using yum to install into the chroot
location dened previously:
# Add Slurm client support meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-slurm-client
# Add Network Time Protocol (NTP) support
[sms]# yum -y --installroot=$CHROOT install ntp
# Add kernel drivers
[sms]# yum -y --installroot=$CHROOT install kernel
# Include modules user environment
[sms]# yum -y --installroot=$CHROOT install lmod-ohpc





Warewulf is congured by default to access an external repository (mirror.centos.org) during the wwmkchroot
process. If the master host cannot reach the public CentOS mirrors, or if you prefer to access a locally
cached mirror, set the $fYUM MIRRORg environment variable to your desired repo location prior to running the
wwmkchroot command below. For example:
# Override default OS repository (optional) - set YUM_MIRROR variable to desired repo location
[sms]# export YUM_MIRROR=${BOS_MIRROR}


# Define chroot location
[sms]# export CHROOT=/opt/ohpc/admin/images/centos7.4
# Build initial chroot image
[sms]# wwmkchroot centos-7 $CHROOT

# Install compute node base meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-base-compute
To access the remote repositories by hostname (and not IP addresses), the chroot environment needs to
be updated to enable DNS resolution. Assuming that the master host has a working DNS conguration in
place, the chroot environment can be updated with a copy of the conguration as follows:
[sms]# cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
Now, we can include additional components to the compute instance using yum to install into the chroot
location dened previously:
# Add Slurm client support meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-slurm-client
# Add Network Time Protocol (NTP) support
[sms]# yum -y --installroot=$CHROOT install ntp
# Add kernel drivers
[sms]# yum -y --installroot=$CHROOT install kernel
# Include modules user environment
[sms]# yum -y --installroot=$CHROOT install lmod-ohpc





Warewulf is congured by default to access an external repository (mirror.centos.org) during the wwmkchroot
process. If the master host cannot reach the public CentOS mirrors, or if you prefer to access a locally
cached mirror, set the $fYUM MIRRORg environment variable to your desired repo location prior to running the
wwmkchroot command below. For example:
# Override default OS repository (optional) - set YUM_MIRROR variable to desired repo location
[sms]# export YUM_MIRROR=${BOS_MIRROR}


# Define chroot location
[sms]# export CHROOT=/opt/ohpc/admin/images/centos7.4
# Build initial chroot image
[sms]# wwmkchroot centos-7 $CHROOT

# Install compute node base meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-base-compute
To access the remote repositories by hostname (and not IP addresses), the chroot environment needs to
be updated to enable DNS resolution. Assuming that the master host has a working DNS conguration in
place, the chroot environment can be updated with a copy of the conguration as follows:
[sms]# cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
Now, we can include additional components to the compute instance using yum to install into the chroot
location dened previously:
# Add Slurm client support meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-slurm-client
# Add Network Time Protocol (NTP) support
[sms]# yum -y --installroot=$CHROOT install ntp
# Add kernel drivers
[sms]# yum -y --installroot=$CHROOT install kernel
# Include modules user environment
[sms]# yum -y --installroot=$CHROOT install lmod-ohpc





Warewulf is congured by default to access an external repository (mirror.centos.org) during the wwmkchroot
process. If the master host cannot reach the public CentOS mirrors, or if you prefer to access a locally
cached mirror, set the $fYUM MIRRORg environment variable to your desired repo location prior to running the
wwmkchroot command below. For example:
# Override default OS repository (optional) - set YUM_MIRROR variable to desired repo location
[sms]# export YUM_MIRROR=${BOS_MIRROR}


# Define chroot location
[sms]# export CHROOT=/opt/ohpc/admin/images/centos7.4
# Build initial chroot image
[sms]# wwmkchroot centos-7 $CHROOT

# Install compute node base meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-base-compute
To access the remote repositories by hostname (and not IP addresses), the chroot environment needs to
be updated to enable DNS resolution. Assuming that the master host has a working DNS conguration in
place, the chroot environment can be updated with a copy of the conguration as follows:
[sms]# cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
Now, we can include additional components to the compute instance using yum to install into the chroot
location dened previously:
# Add Slurm client support meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-slurm-client
# Add Network Time Protocol (NTP) support
[sms]# yum -y --installroot=$CHROOT install ntp
# Add kernel drivers
[sms]# yum -y --installroot=$CHROOT install kernel
# Include modules user environment
[sms]# yum -y --installroot=$CHROOT install lmod-ohpc





Warewulf is congured by default to access an external repository (mirror.centos.org) during the wwmkchroot
process. If the master host cannot reach the public CentOS mirrors, or if you prefer to access a locally
cached mirror, set the $fYUM MIRRORg environment variable to your desired repo location prior to running the
wwmkchroot command below. For example:
# Override default OS repository (optional) - set YUM_MIRROR variable to desired repo location
[sms]# export YUM_MIRROR=${BOS_MIRROR}


# Define chroot location
[sms]# export CHROOT=/opt/ohpc/admin/images/centos7.4
# Build initial chroot image
[sms]# wwmkchroot centos-7 $CHROOT

# Install compute node base meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-base-compute
To access the remote repositories by hostname (and not IP addresses), the chroot environment needs to
be updated to enable DNS resolution. Assuming that the master host has a working DNS conguration in
place, the chroot environment can be updated with a copy of the conguration as follows:
[sms]# cp -p /etc/resolv.conf $CHROOT/etc/resolv.conf
Now, we can include additional components to the compute instance using yum to install into the chroot
location dened previously:
# Add Slurm client support meta-package
[sms]# yum -y --installroot=$CHROOT install ohpc-slurm-client
# Add Network Time Protocol (NTP) support
[sms]# yum -y --installroot=$CHROOT install ntp
# Add kernel drivers
[sms]# yum -y --installroot=$CHROOT install kernel
# Include modules user environment
[sms]# yum -y --installroot=$CHROOT install lmod-ohpc


ustomize system conguration
Prior to assembling the image, it is advantageous to perform any additional customization within the chroot
environment created for the desired compute instance. The following steps document the process to add a
local ssh key created by Warewulf to support remote access, identify the resource manager server, congure
NTP for compute resources, and enable NFS mounting of a $HOME le system and the public OpenHPC
install path (/opt/ohpc/pub) that will be hosted by the master host in this example conguration.
# Initialize warewulf database and ssh_keys
[sms]# wwinit database
[sms]# wwinit ssh_keys
# Add NFS client mounts of /home and /opt/ohpc/pub to base image
[sms]# echo "${sms_ip}:/home /home nfs nfsvers=3,nodev,nosuid,noatime 0 0" >> $CHROOT/etc/fstab
[sms]# echo "${sms_ip}:/opt/ohpc/pub /opt/ohpc/pub nfs nfsvers=3,nodev,noatime 0 0" >> $CHROOT/etc/fstab
# Export /home and OpenHPC public packages from master server
[sms]# echo "/home *(rw,no_subtree_check,fsid=10,no_root_squash)" >> /etc/exports
[sms]# echo "/opt/ohpc/pub *(ro,no_subtree_check,fsid=11)" >> /etc/exports
[sms]# exportfs -a
[sms]# systemctl restart nfs-server
[sms]# systemctl enable nfs-server
# Enable NTP time service on computes and identify master host as local NTP server
[sms]# chroot $CHROOT systemctl enable ntpd
[sms]# echo "server ${sms_ip}" >> $CHROOT/etc/ntp.conf


3.8.4.1 Enable InniBand drivers If your compute resources support InniBand, the following com-
mands add OFED and PSM support using base distro-provided drivers to the compute image.
# Add IB support and enable
[sms]# yum -y --installroot=$CHROOT groupinstall "InfiniBand Support"
[sms]# yum -y --installroot=$CHROOT install infinipath-psm
[sms]# chroot $CHROOT systemctl enable rdma


3.8.4.3 Increase locked memory limits In order to utilize InniBand or Omni-Path as the underlying
high speed interconnect, it is generally necessary to increase the locked memory settings for system users.
This can be accomplished by updating the /etc/security/limits.conf le and this should be performed
within the compute image and on all job submission hosts. In this recipe, jobs are submitted from the master
host, and the following commands can be used to update the maximum locked memory settings on both the
master host and the compute image:
# Update memlock settings on master
[sms]# perl -pi -e 's/# End of file/\* soft memlock unlimited\n$&/s' /etc/security/limits.conf
[sms]# perl -pi -e 's/# End of file/\* hard memlock unlimited\n$&/s' /etc/security/limits.conf
# Update memlock settings within compute image
[sms]# perl -pi -e 's/# End of file/\* soft memlock unlimited\n$&/s' $CHROOT/etc/security/limits.conf
[sms]# perl -pi -e 's/# End of file/\* hard memlock unlimited\n$&/s' $CHROOT/etc/security/limits.conf


3.8.4.4 Enable ssh control via resource manager An additional optional customization that is
recommended is to restrict ssh access on compute nodes to only allow access by users who have an active
job associated with the node. This can be enabled via the use of a pluggable authentication module (PAM)
provided as part of the Slurm package installs. To enable this feature within the compute image, issue the
following:
[sms]# echo "account required pam_slurm.so" >> $CHROOT/etc/pam.d/sshd


3.8.4.6 Add Lustre client To add Lustre client support on the cluster, it necessary to install the client
and associated modules on each host needing to access a Lustre le system. In this recipe, it is assumed
that the Lustre le system is hosted by servers that are pre-existing and are not part of the install process.
Outlining the variety of Lustre client mounting options is beyond the scope of this document, but the general
requirement is to add a mount entry for the desired le system that denes the management server (MGS)
and underlying network transport protocol. To add client mounts on both the master server and compute
image, the following commands can be used. Note that the Lustre le system to be mounted is identied
by the $fmgs fs nameg variable. In this example, the le system is congured to be mounted locally as
/mnt/lustre.
# Add Lustre client software to master host
[sms]# yum -y install lustre-client-ohpc lustre-client-ohpc-modules
# Include Lustre client software in compute image
[sms]# yum -y --installroot=$CHROOT install lustre-client-ohpc lustre-client-ohpc-modules
# Include mount point and file system mount in compute image
[sms]# mkdir $CHROOT/mnt/lustre
[sms]# echo "${mgs_fs_name} /mnt/lustre lustre defaults,_netdev,localflock,retry=2 0 0" >> $CHROOT/etc/fstab
The default underlying network type used by Lustre is tcp. If your external Lustre le system is to be
mounted using a network type other than tcp, additional conguration les are necessary to identify the de-
sired network type. The example below illustrates creation of modprobe conguration les instructing Lustre
to use an InniBand network with the o2ib LNET driver attached to ib0. Note that these modications
are made to both the master host and compute image.
[sms]# echo "options lnet networks=o2ib(ib0)" >> /etc/modprobe.d/lustre.conf
[sms]# echo "options lnet networks=o2ib(ib0)" >> $CHROOT/etc/modprobe.d/lustre.conf
With the Lustre conguration complete, the client can be mounted on the master host as follows:
[sms]# mkdir /mnt/lustre
[sms]# mount -t lustre -o localflock ${mgs_fs_name} /mnt/lustre


3.8.4.7 Enable forwarding of system logs It is often desirable to consolidate system logging infor-
mation for the cluster in a central location, both to provide easy access to the data, and to reduce the impact
of storing data inside the stateless compute node's memory footprint. The following commands highlight
the steps necessary to congure compute nodes to forward their logs to the SMS, and to allow the SMS to
accept these log requests.
# Configure SMS to receive messages and reload rsyslog configuration
[sms]# perl -pi -e "s/\\#\\\$ModLoad imudp/\\\$ModLoad imudp/" /etc/rsyslog.conf
[sms]# perl -pi -e "s/\\#\\\$UDPServerRun 514/\\\$UDPServerRun 514/" /etc/rsyslog.conf
[sms]# systemctl restart rsyslog
# Define compute node forwarding destination
[sms]# echo "*.* @${sms_ip}:514" >> $CHROOT/etc/rsyslog.conf
# Disable most local logging on computes. Emergency and boot logs will remain on the compute nodes
[sms]# perl -pi -e "s/^\*\.info/\\#\*\.info/" $CHROOT/etc/rsyslog.conf
[sms]# perl -pi -e "s/^authpriv/\\#authpriv/" $CHROOT/etc/rsyslog.conf
[sms]# perl -pi -e "s/^mail/\\#mail/" $CHROOT/etc/rsyslog.conf
[sms]# perl -pi -e "s/^cron/\\#cron/" $CHROOT/etc/rsyslog.conf
[sms]# perl -pi -e "s/^uucp/\\#uucp/" $CHROOT/etc/rsyslog.conf

3.8.4.10 Add ClusterShell ClusterShell is an event-based Python library to execute commands in
parallel across cluster nodes. Installation and basic conguration dening three node groups (adm, compute,
and all) is as follows:
# Install ClusterShell
[sms]# yum -y install clustershell-ohpc
# Setup node definitions
17 Rev: 0984468b6
Install Guide (v1.3.3): CentOS7.4/x86 64 + Warewulf + SLURM
[sms]# cd /etc/clustershell/groups.d
[sms]# mv local.cfg local.cfg.orig
[sms]# echo "adm: ${sms_name}" > local.cfg
[sms]# echo "compute: ${compute_prefix}[1-${num_computes}]" >> local.cfg
[sms]# echo "all: @adm,@compute" >> local.cfg


3.8.5 Import les
The Warewulf system includes functionality to import arbitrary les from the provisioning server for distri-
bution to managed hosts. This is one way to distribute user credentials to compute nodes. To import local
le-based credentials, issue the following:
[sms]# wwsh file import /etc/passwd
[sms]# wwsh file import /etc/group
[sms]# wwsh file import /etc/shadow
Similarly, to import the global Slurm conguration le and the cryptographic key that is required by the
munge authentication library to be available on every host in the resource management pool, issue the
following:
[sms]# wwsh file import /etc/slurm/slurm.conf
[sms]# wwsh file import /etc/munge/munge.key
Finally, to add optional support for controlling IPoIB interfaces (see §3.5), OpenHPC includes a template
le for Warewulf that can optionally be imported and used later to provision ib0 network settings.
[sms]# wwsh file import /opt/ohpc/pub/examples/network/centos/ifcfg-ib0.ww
[sms]# wwsh -y file set ifcfg-ib0.ww --path=/etc/sysconfig/network-scripts/ifcfg-ib0


3.9.1 Assemble bootstrap image
The bootstrap image includes the runtime kernel and associated modules, as well as some simple scripts to
complete the provisioning process. The following commands highlight the inclusion of additional drivers and
creation of the bootstrap image based on the running kernel.
# (Optional) Include drivers from kernel updates; needed if enabling additional kernel modules on computes
[sms]# export WW_CONF=/etc/warewulf/bootstrap.conf
[sms]# echo "drivers += updates/kernel/" >> $WW_CONF
# (Optional) Include overlayfs drivers; needed by Singularity
[sms]# echo "drivers += overlay" >> $WW_CONF
# Build bootstrap image
[sms]# wwbootstrap `uname -r`


# Set provisioning interface as the default networking device
[sms]# echo "GATEWAYDEV=${eth_provision}" > /tmp/network.$$
[sms]# wwsh -y file import /tmp/network.$$ --name network
[sms]# wwsh -y file set network --path /etc/sysconfig/network --mode=0644 --uid=0
# Add nodes to Warewulf data store
[sms]# for ((i=0; i<$num_computes; i++)) ; do
wwsh -y node new ${c_name[i]} --ipaddr=${c_ip[i]} --hwaddr=${c_mac[i]} -D ${eth_provision}
done
# Additional step required if desiring to use predictable network interface
# naming schemes (e.g. en4s0f0). Skip if using eth# style names.
[sms]# wwsh provision set "${compute_regex}" --kargs "net.ifnames=1,biosdevname=1"
[sms]# wwsh provision set --postnetdown=1 "${compute_regex}"
# Define provisioning image for hosts
[sms]# wwsh -y provision set "${compute_regex}" --vnfs=centos7.4 --bootstrap=`uname -r` \
--files=dynamic_hosts,passwd,group,shadow,slurm.conf,munge.key,network
# Optionally define IPoIB network settings (required if planning to mount Lustre/BeeGFS over IB)
[sms]# for ((i=0; i<$num_computes; i++)) ; do
wwsh -y node set ${c_name[$i]} -D ib0 --ipaddr=${c_ipoib[$i]} --netmask=${ipoib_netmask}
done
[sms]# wwsh -y provision set "${compute_regex}" --fileadd=ifcfg-ib0.ww


Warewulf includes a utility named wwnodescan to automatically register new compute nodes versus the
outlined node-addition approach which requires hardware MAC addresses to be gathered in advance. With
wwnodescan, nodes will be added to the Warewulf database in the order in which their DHCP requests are
received by the master, so care must be taken to boot nodes in the order one wishes to see preserved in the
Warewulf database. The IP address provided will be incremented after each node is found, and the utility
will exit after all specied nodes have been found. Example usage is highlighted below:
[sms]# wwnodescan --netdev=${eth_provision} --ipaddr=${c_ip[0]} --netmask=${internal_netmask} \
--vnfs=centos7.4 --bootstrap=`uname -r` --listen=${sms_eth_internal} ${c_name[0]}-${c_name[3]}
# Restart dhcp / update PXE
[sms]# systemctl restart dhcpd
[sms]# wwsh pxe update
3.9.4 Optional kernel arguments
If you chose to enable ConMan in §3.8.4.13, additional boot-time kernel arguments are needed to enable serial
console redirection. An example provisioning setting which adds to any other kernel arguments dened in
$fkargsg is as follows:
# Define node kernel arguments to support SOL console
[sms]# wwsh -y provision set "${compute_regex}" --console=ttyS1,115200
3.9.5 Optionally congure stateful provisioning
Warewulf normally defaults to running the assembled VNFS image out of system memory in a stateless
conguration. Alternatively, Warewulf can also be used to partition and format persistent storage such that
the VNFS image can be installed locally to disk in a stateful manner. This does, however, require that a
boot loader (GRUB) be added to the image as follows:
# Add GRUB2 bootloader and re-assemble VNFS image
[sms]# yum -y --installroot=$CHROOT install grub2
[sms]# wwvnfs --chroot $CHROOT
Enabling stateful nodes also requires additional site-specic, disk-related parameters in the Warewulf con-
guration, and several example partitioning scripts are provided in the distribution.
# Select (and customize) appropriate parted layout example
[sms]# cp /etc/warewulf/filesystem/examples/gpt_example.cmds /etc/warewulf/filesystem/gpt.cmds
[sms]# wwsh provision set --filesystem=gpt "${compute_regex}"
[sms]# wwsh provision set --bootloader=sda "${compute_regex}"


Those provisioning compute nodes in UEFI mode will install a slightly dierent set of packages in to the
VNFS. Warewulf also provides an example EFI lesystem layout.
# Add GRUB2 bootloader and re-assemble VNFS image
[sms]# yum -y --installroot=$CHROOT install grub2-efi grub2-efi-modules
[sms]# cp /etc/warewulf/filesystem/examples/efi_example.cmds /etc/warewulf/filesystem/efi.cmds
[sms]# wwsh provision set --filesystem=efi "${compute_regex}"
[sms]# wwsh provision set --bootloader=sda "${compute_regex}"

4 Install OpenHPC Development Components
The install procedure outlined in §3 highlighted the steps necessary to install a master host, assemble
and customize a compute image, and provision several compute hosts from bare-metal. With these steps
completed, additional OpenHPC-provided packages can now be added to support a 
exible HPC development
environment including development tools, C/C++/Fortran compilers, MPI stacks, and a variety of 3rd party
libraries. The following subsections highlight the additional software installation procedures.
4.1 Development Tools
To aid in general development eorts, OpenHPC provides recent versions of the GNU autotools collection,
the Valgrind memory debugger, EasyBuild, and Spack. These can be installed as follows:
# Install autotools meta-package
[sms]# yum -y install ohpc-autotools
[sms]# yum -y install EasyBuild-ohpc
[sms]# yum -y install hwloc-ohpc
[sms]# yum -y install spack-ohpc
[sms]# yum -y install valgrind-ohpc
4.2 Compilers
OpenHPC presently packages the GNU compiler toolchain integrated with the underlying modules-environment
system in a hierarchical fashion. The modules system will conditionally present compiler-dependent software
based on the toolchain currently loaded.
[sms]# yum -y install gnu7-compilers-ohpc
The llvm compiler toolchains are also provided as a standalone additional compiler family (ie. users can
easily switch between gcc/clang environments), but we do not provide the full complement of downstream
library builds.
[sms]# yum -y install llvm5-compilers-ohpc
4.3 MPI Stacks
For MPI development and runtime support, OpenHPC provides pre-packaged builds for a variety of MPI
families and transport layers. Currently available options and their applicability to various network trans-
ports are summarized in Table 1. The command that follows installs a starting set of MPI families that are
compatible with ethernet fabrics.

[sms]# yum -y install openmpi3-gnu7-ohpc mpich-gnu7-ohpc
If your system includes InniBand and you enabled underlying support in §3.5 and §3.8.4, an additional
MVAPICH2 family is available for use:
[sms]# yum -y install mvapich2-gnu7-ohpc
Alternatively, if your system includes Intel® Omni-Path, use the (psm2) variant of MVAPICH2 instead:
[sms]# yum -y install mvapich2-psm2-gnu7-ohpc
An additional OpenMPI build variant is listed in Table 1 which enables PMIx job launch support for use
with Slurm. This optional variant is available as openmpi3-pmix-slurm-gnu7-ohpc.
4.4 Performance Tools
OpenHPC provides a variety of open-source tools to aid in application performance analysis (refer to Ap-
pendix E for a listing of available packages). This group of tools can be installed as follows:
# Install perf-tools meta-package
[sms]# yum -y install ohpc-gnu7-perf-tools
4.5 Setup default development environment
System users often nd it convenient to have a default development environment in place so that compilation
can be performed directly for parallel programs requiring MPI. This setup can be conveniently enabled via
modules and the OpenHPC modules environment is pre-congured to load an ohpc module on login (if
present). The following package install provides a default environment that enables autotools, the GNU
compiler toolchain, and the OpenMPI stack.
[sms]# yum -y install lmod-defaults-gnu7-openmpi3-ohpc



4.6 3rd Party Libraries and Tools
OpenHPC provides pre-packaged builds for a number of popular open-source tools and libraries used by HPC
applications and developers. For example, OpenHPC provides builds for FFTW and HDF5 (including serial
and parallel I/O support), and the GNU Scientic Library (GSL). Again, multiple builds of each package
are available in the OpenHPC repository to support multiple compiler and MPI family combinations where
appropriate. Note, however, that not all combinatorial permutations may be available for components where
there are known license incompatibilities. The general naming convention for builds provided by OpenHPC
is to append the compiler and MPI family name that the library was built against directly into the package
24 Rev: 0984468b6
Install Guide (v1.3.3): CentOS7.4/x86 64 + Warewulf + SLURM
name. For example, libraries that do not require MPI as part of the build process adopt the following RPM
name:
package-<compiler family>-ohpc-<package version>-<release>.rpm
Packages that do require MPI as part of the build expand upon this convention to additionally include the
MPI family name as follows:
package-<compiler family>-<mpi family>-ohpc-<package version>-<release>.rpm
To illustrate this further, the command below queries the locally congured repositories to identify all of
the available PETSc packages that were built with the GNU toolchain. The resulting output that is included
shows that pre-built versions are available for each of the supported MPI families presented in §4.3.
[sms]# yum search petsc-gnu7 ohpc
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
=========================== N/S matched: petsc-gnu7, ohpc ===========================
petsc-gnu7-impi-ohpc.x86_64 : Portable Extensible Toolkit for Scientific Computation
petsc-gnu7-mpich-ohpc.x86_64 : Portable Extensible Toolkit for Scientific Computation
petsc-gnu7-mvapich2-ohpc.x86_64 : Portable Extensible Toolkit for Scientific Computation
petsc-gnu7-openmpi3-ohpc.x86_64 : Portable Extensible Toolkit for Scientific Computation
Tip
OpenHPC-provided 3rd party builds are congured to be installed into a common top-level repository so that
they can be easily exported to desired hosts within the cluster. This common top-level path (/opt/ohpc/pub)
was previously congured to be mounted on compute nodes in §3.8.3, so the packages will be immediately
available for use on the cluster after installation on the master host.
For convenience, OpenHPC provides package aliases for these 3rd party libraries and utilities that can
be used to install available libraries for use with the GNU compiler family toolchain. For parallel libraries,
aliases are grouped by MPI family toolchain so that administrators can choose a subset should they favor a
particular MPI stack. Please refer to Appendix E for a more detailed listing of all available packages in each
of these functional areas. To install all available package oerings within OpenHPC, issue the following:
# Install 3rd party libraries/tools meta-packages built with GNU toolchain
[sms]# yum -y install ohpc-gnu7-serial-libs
[sms]# yum -y install ohpc-gnu7-io-libs
[sms]# yum -y install ohpc-gnu7-python-libs
[sms]# yum -y install ohpc-gnu7-runtimes
# Install parallel lib meta-packages for all available MPI toolchains
[sms]# yum -y install ohpc-gnu7-mpich-parallel-libs
[sms]# yum -y install ohpc-gnu7-openmpi3-parallel-libs
4.7 Optional Development Tool Builds
In addition to the 3rd party development libraries built using the open source toolchains mentioned in §4.6,
OpenHPC also provides optional compatible builds for use with the compilers and MPI stack included in
25 Rev: 0984468b6
Install Guide (v1.3.3): CentOS7.4/x86 64 + Warewulf + SLURM
newer versions of the Intel® Parallel Studio XE software suite. These packages provide a similar hierarchical
user environment experience as other compiler and MPI families present in OpenHPC.
To take advantage of the available builds, the Parallel Studio software suite must be obtained and installed
separately. Once installed locally, the OpenHPC compatible packages can be installed using standard package
manager semantics. Note that licenses are provided free of charge for many categories of use. In particular,
licenses for compilers and developments tools are provided at no cost to academic researchers or developers
contributing to open-source software projects. More information on this program can be found at:
https://software.intel.com/en-us/qualify-for-free-software
Tip
As noted in §3.8.3, the default installation path for OpenHPC (/opt/ohpc/pub) is exported over
NFS from the master to the compute nodes, but the Parallel Studio installer defaults to a path of
/opt/intel. To make the Intel® compilers available to the compute nodes one must either customize
the Parallel Studio installation path to be within /opt/ohpc/pub, or alternatively, add an additional
NFS export for /opt/intel that is mounted on desired compute nodes.
To enable all 3rd party builds available in OpenHPC that are compatible with Intel® Parallel Studio, issue
the following:
# Install OpenHPC compatibility packages (requires prior installation of Parallel Studio)
[sms]# yum -y install intel-compilers-devel-ohpc
[sms]# yum -y install intel-mpi-devel-ohpc
# Optionally, choose Omni-Path enabled MPI builds. Otherwise, skip to retain default MPI stacks
[sms]# yum -y install openmpi-psm2-intel-ohpc mvapich2-psm2-intel-ohpc
# Install 3rd party libraries/tools meta-packages built with Intel toolchain
[sms]# yum -y install ohpc-intel-serial-libs
[sms]# yum -y install ohpc-intel-io-libs
[sms]# yum -y install ohpc-intel-perf-tools
[sms]# yum -y install ohpc-intel-python-libs
[sms]# yum -y install ohpc-intel-runtimes
[sms]# yum -y install ohpc-intel-mpich-parallel-libs
[sms]# yum -y install ohpc-intel-mvapich2-parallel-libs
[sms]# yum -y install ohpc-intel-openmpi3-parallel-libs
[sms]# yum -y install ohpc-intel-impi-parallel-libs


5 Resource Manager Startup
In section §3, the Slurm resource manager was installed and congured for use on both the master host and
compute node instances. With the cluster nodes up and functional, we can now startup the resource manager
services in preparation for running user jobs. Generally, this is a two-step process that requires starting up
the controller daemons on the master host and the client daemons on each of the compute hosts. Note that
Slurm leverages the use of the munge library to provide authentication services and this daemon also needs
to be running on all hosts within the resource management pool. The following commands can be used to
startup the necessary services to support resource management under Slurm.
# Start munge and slurm controller on master host
[sms]# systemctl enable munge
[sms]# systemctl enable slurmctld
[sms]# systemctl start munge
[sms]# systemctl start slurmctld
# Start slurm clients on compute hosts
[sms]# pdsh -w $compute_prefix[1-4] systemctl start slurmd
6 Run a Test Job
With the resource manager enabled for production usage, users should now be able to run jobs. To demon-
strate this, we will add a \test" user on the master host that can be used to run an example job.
[sms]# useradd -m test
Warewulf installs a utility on the compute nodes to automatically synchronize known les from the
provisioning server at ve minute intervals. In this recipe, recall that we previously registered credential les
with Warewulf (e.g. passwd, group, and shadow) so that these les would be propagated during compute
node imaging. However, with the addition of a new \test" user above, the les have been outdated and we
need to update theWarewulf database to incorporate the

